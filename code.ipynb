{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"2.pdf_chat_router_issue_assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "from langchain import HuggingFaceHub\n",
    "# model_name=\"Locutusque/gpt2-xl-conversational\"\n",
    "# model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_name = \"llmware/bling-sheared-llama-1.3b-0.1\"\n",
    "# model_name = \"llmware/bling-falcon-1b-0.1\"\n",
    "\n",
    "llm= HuggingFaceHub(repo_id=model_name, model_kwargs={\"temperature\":0.5,\"max_length\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.6\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arasu\\Workspace\\Projects\\GenAI\\2.pdf_chat_router_issue_assistant\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model = \"C:/Users/arasu/Workspace/Projects/GenAI/models/MBZUAILaMini-Flan-T5-248M/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model,truncation=True)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "pipe = pipeline(\n",
    "        'text2text-generation',\n",
    "        model = base_model,\n",
    "        tokenizer = tokenizer,\n",
    "        max_length = 500,\n",
    "        do_sample = True,\n",
    "        temperature = 0.3,\n",
    "        top_p= 0.95\n",
    "    )\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arasu\\Workspace\\Projects\\GenAI\\2.pdf_chat_router_issue_assistant\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Entities: - Name: Arasu - Email: arasucanada@gmail.com - Address: 620 67 Ave SW - Zip code: t2v0m2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = f\"\"\" Extract the entities and their types from the below pharse in json format. \n",
    "pharse : name-arasu,email- arasucanada@gmail.com,address-620 67 ave sw,zip code- t2v0m2\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "pdf = PdfReader(\"employment-agreement2018.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "raw_text = \"\"\n",
    "for i, page in enumerate(pdf.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='C:/Users/arasu/Workspace/Projects/GenAI/embeddings/sentence-transformers_all-mpnet-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"C:/Users/arasu/Workspace/Projects/GenAI/embeddings/hkunlp_instructor-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db_path = \"vector_db\"\n",
    "vector_db = Chroma.from_texts(texts,instructor_embeddings,persist_directory=db_path)\n",
    "vector_db.persist()\n",
    "vector_db = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db_path = \"vector_db\"\n",
    "vector_db = Chroma(persist_directory=db_path,embedding_function=instructor_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"how many annual leaves am i eligible?\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=90): #preserve_newlines\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model  = \"google-t5/t5-small\"\n",
    "summarize_tokenizer  = AutoTokenizer.from_pretrained(model,truncation=True)\n",
    "summarize_model  = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "pipe_summary = pipeline(\"summarization\", model=summarize_model, tokenizer=summarize_tokenizer) #, max_new_tokens=500, min_new_tokens=300\n",
    "hf_summary = HuggingFacePipeline(pipeline=pipe_summary)\n",
    "memory = ConversationSummaryBufferMemory(llm=hf_summary, memory_key=\"chat_history\", input_key=\"human_input\", max_token_limit=300, human_prefix = \"\", ai_prefix = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = \"\"\"You are a chatbot having a conversation with a human. your job is to help providing the best answer for questions on Indian consititution.\n",
    "  Use only information in the following paragraphs to answer the question at the end. Explain the answer with reference to these paragraphs.\n",
    "  If you don't have relavent information in paragraph then give response \"I dont't know\".\n",
    "\n",
    "  {context}\n",
    "\n",
    "  {chat_history}\n",
    "\n",
    "  {human_input}\n",
    "\n",
    "  Response:\n",
    "  \"\"\"\n",
    "prompt = PromptTemplate(input_variables=['context', 'human_input', 'chat_history'], template=template)\n",
    "\n",
    "load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=prompt, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# Instantiate an object of Conversation Buffer Memory\n",
    "# chat_history = ConversationBufferWindowMemory(memory_key=\"chat_history\", \n",
    "# \t\t\t\t\t\t\t\t\t\treturn_messages=True,k=5\n",
    "                                        \n",
    "# \t\t\t\t\t\t\t\t\t\t)\n",
    "conv_qa = ConversationalRetrievalChain.from_llm(llm=llm,  # USing ChatOpenAI as the LLM\n",
    "                                                retriever=retriever,  # # set the vectorstore to do similarity search\n",
    "                                                memory=memory,\n",
    "                                                 # Provide the buffer memory object to pass the conversation history to LLM.\n",
    "                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qa({\"question\":\"am i eligible for a Superannuation?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(\"hi\")\n",
    "print(wrap_text(output['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are friendly chatbot trying to help user based only on the context provided.\\\n",
    "if the question contains greetings then greet the user back. be friendly.\\\n",
    "if the answer is not found in the context then reply \"No Evidence Found\".\\\n",
    "do not make up answers.\\\n",
    "context: {context}\n",
    "question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm,\n",
    "                            chain_type=\"stuff\",\n",
    "                            retriever=retriever,\n",
    "                            input_key=\"query\",\n",
    "                            return_source_documents=True,\n",
    "                            chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain(\"what is my monthly fixed salary?\")\n",
    "print(wrap_text(output['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "model  = \"google-t5/t5-small\"\n",
    "summarize_tokenizer  = AutoTokenizer.from_pretrained(model,truncation=True)\n",
    "summarize_model  = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "pipe_summary = pipeline(\"summarization\", model=summarize_model, tokenizer=summarize_tokenizer) #, max_new_tokens=500, min_new_tokens=300\n",
    "hf_summary = HuggingFacePipeline(pipeline=pipe_summary)\n",
    "memory = ConversationSummaryBufferMemory(llm=hf_summary, memory_key=\"chat_history\", input_key=\"human_input\", max_token_limit=300, human_prefix = \"\", ai_prefix = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
